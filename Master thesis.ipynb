{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting link: https://www.sec.gov/Archives/edgar/data/1047335/000143774920023548/0001437749-20-023548-index.htm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scrapy\n",
      "  Downloading Scrapy-2.4.0-py2.py3-none-any.whl (239 kB)\n",
      "\u001b[K     |████████████████████████████████| 239 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting queuelib>=1.4.2\n",
      "  Downloading queuelib-1.5.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting w3lib>=1.17.0\n",
      "  Downloading w3lib-1.22.0-py2.py3-none-any.whl (20 kB)\n",
      "Collecting Twisted>=17.9.0\n",
      "  Downloading Twisted-20.3.0-cp37-cp37m-macosx_10_6_intel.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting PyDispatcher>=2.0.5\n",
      "  Downloading PyDispatcher-2.0.5.tar.gz (34 kB)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in /opt/anaconda3/lib/python3.7/site-packages (from scrapy) (19.1.0)\n",
      "Collecting parsel>=1.5.0\n",
      "  Downloading parsel-1.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting zope.interface>=4.1.3\n",
      "  Downloading zope.interface-5.1.2-cp37-cp37m-macosx_10_9_x86_64.whl (193 kB)\n",
      "\u001b[K     |████████████████████████████████| 193 kB 3.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protego>=0.1.15\n",
      "  Downloading Protego-0.1.16.tar.gz (3.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2 MB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting service-identity>=16.0.0\n",
      "  Downloading service_identity-18.1.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting itemadapter>=0.1.0\n",
      "  Downloading itemadapter-0.1.1-py3-none-any.whl (7.7 kB)\n",
      "Requirement already satisfied: cryptography>=2.0 in /opt/anaconda3/lib/python3.7/site-packages (from scrapy) (2.8)\n",
      "Requirement already satisfied: lxml>=3.5.0; platform_python_implementation == \"CPython\" in /opt/anaconda3/lib/python3.7/site-packages (from scrapy) (4.5.0)\n",
      "Collecting itemloaders>=1.0.1\n",
      "  Downloading itemloaders-1.0.3-py3-none-any.whl (11 kB)\n",
      "Collecting cssselect>=0.9.1\n",
      "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: six>=1.4.1 in /opt/anaconda3/lib/python3.7/site-packages (from w3lib>=1.17.0->scrapy) (1.14.0)\n",
      "Collecting PyHamcrest!=1.10.0,>=1.9.0\n",
      "  Downloading PyHamcrest-2.0.2-py3-none-any.whl (52 kB)\n",
      "\u001b[K     |████████████████████████████████| 52 kB 769 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting incremental>=16.10.1\n",
      "  Downloading incremental-17.5.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting hyperlink>=17.1.1\n",
      "  Downloading hyperlink-20.0.1-py2.py3-none-any.whl (48 kB)\n",
      "\u001b[K     |████████████████████████████████| 48 kB 490 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=19.2.0 in /opt/anaconda3/lib/python3.7/site-packages (from Twisted>=17.9.0->scrapy) (19.3.0)\n",
      "Collecting constantly>=15.1\n",
      "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting Automat>=0.3.0\n",
      "  Downloading Automat-20.2.0-py2.py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.7/site-packages (from zope.interface>=4.1.3->scrapy) (46.0.0.post20200309)\n",
      "Collecting pyasn1\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1-modules\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 5.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cffi!=1.11.3,>=1.8 in /opt/anaconda3/lib/python3.7/site-packages (from cryptography>=2.0->scrapy) (1.14.0)\n",
      "Collecting jmespath>=0.9.5\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: idna>=2.5 in /opt/anaconda3/lib/python3.7/site-packages (from hyperlink>=17.1.1->Twisted>=17.9.0->scrapy) (2.8)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0->scrapy) (2.19)\n",
      "Building wheels for collected packages: PyDispatcher, protego\n",
      "  Building wheel for PyDispatcher (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-py3-none-any.whl size=11515 sha256=9d8c68170f2a1f623ac4231a0895e2973e5acd3686bf5f82bda4e8ffe1f96f46\n",
      "  Stored in directory: /Users/danielbejarano/Library/Caches/pip/wheels/dc/d0/bf/0cc715c01fce0bace63b46283acf5cc630d5e5dbb4602c54e5\n",
      "  Building wheel for protego (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for protego: filename=Protego-0.1.16-py3-none-any.whl size=7765 sha256=ffd15c6a00a4e5471479f9f4557ace0d175aa080a70372086e2a58e85d564b2b\n",
      "  Stored in directory: /Users/danielbejarano/Library/Caches/pip/wheels/ca/44/01/3592ccfbcfaee4ab297c4097e6e9dbe1c7697e3531a39877ab\n",
      "Successfully built PyDispatcher protego\n",
      "Installing collected packages: queuelib, w3lib, PyHamcrest, incremental, hyperlink, zope.interface, constantly, Automat, Twisted, PyDispatcher, cssselect, parsel, protego, pyasn1, pyasn1-modules, service-identity, itemadapter, jmespath, itemloaders, scrapy\n",
      "Successfully installed Automat-20.2.0 PyDispatcher-2.0.5 PyHamcrest-2.0.2 Twisted-20.3.0 constantly-15.1.0 cssselect-1.1.0 hyperlink-20.0.1 incremental-17.5.0 itemadapter-0.1.1 itemloaders-1.0.3 jmespath-0.10.0 parsel-1.6.0 protego-0.1.16 pyasn1-0.4.8 pyasn1-modules-0.2.8 queuelib-1.5.0 scrapy-2.4.0 service-identity-18.1.0 w3lib-1.22.0 zope.interface-5.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -- scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General info on 8K publication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives:\n",
    "* Scrape company name and CIK number from the filing details\n",
    "* Follow the link to the actual 8K publication\n",
    "* Scrape the title of the first item of the publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy import Selector\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import requests\n",
    "import pandas as pd\n",
    "import logging\n",
    "import json\n",
    "from urllib.parse import urljoin, urlparse\n",
    "print(\"Libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json pipeline that will turn the scraped contents into a json file\n",
    "class JsonWriterPipeline(object):\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('8kform.jl', 'w')\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spider\n",
    "class Spider_8k(scrapy.Spider):\n",
    "    name = '8kform'\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1}, \n",
    "        'FEED_FORMAT':'json',                                 \n",
    "        'FEED_URI': '8kform.json'                        \n",
    "    }\n",
    "    \n",
    "    def start_requests(self):\n",
    "        url = 'https://www.sec.gov/Archives/edgar/data/1047335/000143774920023548/0001437749-20-023548-index.htm'\n",
    "        yield scrapy.Request(url=url, callback=self.parse)\n",
    "        \n",
    "    # Here I am going to scrape the company info and follow the link to the 8K publication\n",
    "    def parse(self, response):\n",
    "        base_url = \"https://www.sec.gov/\"\n",
    "        next_page = response.css('#formDiv > div > table > tbody > tr:nth-child(2) > td:nth-child(3) > a::attr(href)').extract()\n",
    "        yield {\n",
    "            'Company': response.xpath('//*[@id=\"filerDiv\"]/div[3]/span/text()[1]').extract_first(),\n",
    "            'CIK': response.xpath('//*[@id=\"filerDiv\"]/div[3]/span/a/text()').extract_first()\n",
    "        }\n",
    "        yield scrapy.Request(url=urljoin(base_url, next_page), callback = self.parse2)\n",
    "        \n",
    "        \n",
    "    # Here I scrape the title of an item in the publication\n",
    "    def parse2(self, response):\n",
    "        yield {\n",
    "            'Title': response.xpath('//*[@id=\"fact-identifier-4\"]/b/text()').extract_first()\n",
    "        }\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-13 11:50:34 [scrapy.utils.log] INFO: Scrapy 2.4.0 started (bot: scrapybot)\n",
      "2020-11-13 11:50:34 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.9, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.7.6 (default, Jan  8 2020, 13:42:34) - [Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Darwin-17.0.0-x86_64-i386-64bit\n",
      "2020-11-13 11:50:34 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2020-11-13 11:50:34 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 30,\n",
      " 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2020-11-13 11:50:34 [py.warnings] WARNING: /opt/anaconda3/lib/python3.7/site-packages/scrapy/extensions/feedexport.py:247: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details\n",
      "  exporter = cls(crawler)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "})\n",
    "process.crawl(Spider_8k)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying only with the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I'll just try to scrape the title of an item in the actual 8K publication, to see if my mistake was scraping the title, or following the link. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy import Selector\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import requests\n",
    "import pandas as pd\n",
    "import logging\n",
    "import json\n",
    "from urllib.parse import urljoin, urlparse\n",
    "print(\"Libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json pipeline that will turn the scraped contents into a json file\n",
    "class JsonWriterPipeline(object):\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('8kform.jl', 'w')\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Spider_8k(scrapy.Spider):\n",
    "    name = '8kform'\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1}, \n",
    "        'FEED_FORMAT':'json',                                 \n",
    "        'FEED_URI': '8kform.json'\n",
    "    }\n",
    "        \n",
    "    def start_requests(self):\n",
    "        url = 'https://www.sec.gov/ix?doc=/Archives/edgar/data/1047335/000143774920023548/nhc20201111_8k.htm'\n",
    "        yield scrapy.Request(url=url, callback = self.parse)\n",
    "        \n",
    "    def parse(self, response):\n",
    "        yield {\n",
    "            'Item': response.xpath('//*[@id=\"dynamic-xbrl-form\"]/div[47]/b/text()').extract_first()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-13 11:56:26 [scrapy.utils.log] INFO: Scrapy 2.4.0 started (bot: scrapybot)\n",
      "2020-11-13 11:56:27 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.9, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.7.6 (default, Jan  8 2020, 13:42:34) - [Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Darwin-17.0.0-x86_64-i386-64bit\n",
      "2020-11-13 11:56:27 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2020-11-13 11:56:27 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 30,\n",
      " 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2020-11-13 11:56:27 [py.warnings] WARNING: /opt/anaconda3/lib/python3.7/site-packages/scrapy/extensions/feedexport.py:247: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details\n",
      "  exporter = cls(crawler)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "})\n",
    "process.crawl(Spider_8k)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
